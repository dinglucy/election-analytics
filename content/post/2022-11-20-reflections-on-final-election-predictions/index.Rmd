---
title: Reflections on Final Election Predictions
author: R package build
date: '2022-11-20'
slug: []
categories: []
tags: []
---

```{r, include = FALSE}
# Hiding all code output
knitr::opts_chunk$set(echo = FALSE)

# Loading libraries
library(tidyverse)
library(janitor)
library(glmnet)
library(sf)
library(plotly)
library(usmap)
library(rmapshaper)
library(blogdown)
library(gridExtra)
library(stargazer)
library(lubridate)
library(caret)
library(leaps)
library(ggthemes)
library(usdata)
library(gt)
library(gtsummary)
library(grid)
library(Metrics)
library(donnermap)
library(scales)
library(lfe)
library(fixest)
library(plotly)
library(gapminder)
library(htmlwidgets)
```

```{r, include = FALSE, message = FALSE, warning = FALSE}
dat <- read_csv("2022_4_0_3.csv", skip = 2, col_names = FALSE)
colnames(dat) <- read_csv("2022_4_0_3.csv", col_names = FALSE, n_max = 1)

##results to supplement David Leip's data from https://www.nytimes.com/interactive/2022/11/08/us/elections/results-house.html?action=click&pgtype=Article&state=default&module=election-results&context=election_recirc&region=NavBar
##As of 11/21/22

##hand enter MA, ME, and MS
dat[dat$FIPS == 25901,'Democratic'] = 153402; dat[dat$FIPS == 25901,'Republican'] = 96499
dat[dat$FIPS == 25902,'Democratic'] = 178472; dat[dat$FIPS == 25902,'Republican'] = 91100
dat[dat$FIPS == 25903,'Democratic'] = 145507; dat[dat$FIPS == 25903,'Republican'] = 82628
dat[dat$FIPS == 25904,'Democratic'] = 0; dat[dat$FIPS == 25904,'Republican'] = 0
dat[dat$FIPS == 25905,'Democratic'] = 198617; dat[dat$FIPS == 25905,'Republican'] = 70694
dat[dat$FIPS == 25906,'Democratic'] = 190062; dat[dat$FIPS == 25906,'Republican'] = 107496
dat[dat$FIPS == 25907,'Democratic'] = 144902; dat[dat$FIPS == 25907,'Republican'] = 26481
dat[dat$FIPS == 25908,'Democratic'] = 184084; dat[dat$FIPS == 25908,'Republican'] = 80961
dat[dat$FIPS == 25909,'Democratic'] = 193426; dat[dat$FIPS == 25909,'Republican'] = 131936

dat[dat$FIPS == 23901,'Democratic'] = 218630; dat[dat$FIPS == 23901,'Republican'] = 128996
dat[dat$FIPS == 23902,'Democratic'] = 151440; dat[dat$FIPS == 23902,'Republican'] = 140895

dat[dat$FIPS == 28901,'Democratic'] = 45222; dat[dat$FIPS == 28901,'Republican'] = 122122
dat[dat$FIPS == 28902,'Democratic'] = 107071; dat[dat$FIPS == 28902,'Republican'] = 71380
dat[dat$FIPS == 28903,'Democratic'] = 54422; dat[dat$FIPS == 28903,'Republican'] = 132269
dat[dat$FIPS == 28904,'Democratic'] = 42876; dat[dat$FIPS == 28904,'Republican'] = 127813

##LA and FL races with no reporting b/c no contest
dat[dat$FIPS == 22904,'Democratic'] = 0; dat[dat$FIPS == 22904,'Republican'] = 0
dat[dat$FIPS == 12905,'Democratic'] = 0; dat[dat$FIPS == 12905,'Republican'] = 0

#Fill in Utah 3rd
dat[dat$FIPS == 49903,'Democratic'] = 82865; dat[dat$FIPS == 49903,'Republican'] = 181141

# Fill in MA
dat[dat$FIPS == 25904, 'RANKD'] = 1

dat <- dat %>% 
  clean_names() %>%
  mutate(cd = ifelse(geographic_name == "At Large", "00", cd)) %>%
  mutate(st_cd_fips = paste0(str_pad(state_fips, 2, pad = "0"), cd)) %>%
  mutate(democratic = democratic + ca_democratic,
         republican = republican + ca_republican) %>%
  mutate(dem_2p = (democratic/(democratic + republican))*100) %>%
  mutate(dem_2p = ifelse(democratic + republican == 0, rankd*100, dem_2p)) %>%
  mutate(winner_d = ifelse(dem_2p > 50, 1, 0))

preds <- read_csv("preds.csv") %>%
  select(-c("...1", "...2", "...3", "...4"))

df <- left_join(dat, preds, by = "st_cd_fips")
df_na <- df %>%
  drop_na(dem_2p, adj_pred)
```

# Introduction

Now that the election is over, it is possible to evaluate how my model has done when it comes to the election. First, I'll take a look at overall measures of accuracy, then look at district-level accuracy, then explore trends in the misses and the correct calls that I made for the election.

# Model Evaluation

To begin with, there is an important error that I only caught after the election itself. I did not predict the eight new districts created this election through redistricting - instead mistakenly predicting for eight older districts that now no longer exist. First, I'll evaluate my model by ignoring these districts, then, I'll predict these districts as I would have with my final model and do model evaluation in that way.

First, when evaluating the model without the eight unpredicted districts, we find that the root mean squared error hovers around 10 for the model with redistricting and 13 for the model without the redistricting adjustment. This means that the weighted average error between the predictors and the actuals is 10 points, which is okay given that the predictions are on a range of 0-100. We do find here that the redistricting model performed better than the model without the redistricting adjustment. As a comparison, both models outperformed directly using the previous vote share for the district in the last election, but the pooled model without redistricting actually did worse in comparison to using just the straight partisan lean for the district. The modeled predictions with the redistricting adjustment did narrowly better than just using the straight partisan lean for the district.

```{r}
rmses <- c(rmse(df_na$dem_2p, df_na$adj_pred), rmse(df_na$dem_2p, df_na$fit), rmse(df_na$dem_2p, df_na$lwr), rmse(df_na$dem_2p, df_na$upr), rmse(df_na$dem_2p, df_na$dem_2p_avg), rmse(df_na$dem_2p, df_na$prev_results_dist))

rmses_names <- c("Predictions w/ Redistricting Adjustment", 
                 "Predictions w/o Redistricting Adjustment", "Lower Prediction Bound",
                 "Higher Prediction Bound", "Average Partisan Lean (Control)", "Previous Results (Control)")

table_rmses <- data.frame(rmses_names, rmses)
colnames(table_rmses) <- c("Model", "RMSE")
table_rmses %>%
  gt() %>%
  tab_header(title = "RMSEs for Each Model Created", subtitle = "Disregarding 8 Unpredicted Districts") %>%
  fmt_number(columns = c("RMSE"))
```

The additional predictions, because they are made in districts with no previous history, entirely consist of using the average partisan lean from the old districts that they were made up of. Many of the predictions come very close to the actual results and the only district that is predicted incorrectly is Florida's 28th district.

```{r, message = FALSE}
additional_preds <- read_csv("additional_preds.csv")
additional_preds %>%
  gt() %>%
  tab_header(title = "Districts Not Predicted",
             subtitle = "Prediction of Democratic Two-Party Vote-Share") %>%
  cols_label(
    st_cd_fips = "State & CD Fips", 
    dem_2p = "Actual Results", 
    adj_pred = "Prediction") %>%
  cols_align(align = "center") %>%
  fmt_percent(columns = c(dem_2p, adj_pred), scale_values = FALSE)
```

The final results of my prediction after correcting these errors is actually further from the actual results. Here I predict that there will be 219 Democratic seats and 216 Republican seats, as opposed to the original incorrect 217D-218R seat prediction. The root mean squared error with these corrections is 9.89, which is narrowly better than the 9.93 in the model without corrections.

# District-Level Accuracy

My final model with redistricting adjustments miscalled 17 districts (~4% of the districts). In 12 of these districts, I was overconfident in the direction of Democrats. Some of these misses were relatively close - about 1-2% off - and could be largely attributed to the fact that these district were swing districts. There were a couple of districts, however, where I was off as many as 18 percentage points. There also appears to be some systematic misses. In particular, I tended to miss a lot of districts in New York and California, predicting them to be substantially more Democratic than they actually were.

```{r}
df <- df %>%
  left_join(additional_preds, by = "st_cd_fips") %>%
  mutate(dem_2p = coalesce(dem_2p.x, dem_2p.y),
         adj_pred = coalesce(adj_pred.x, adj_pred.y)) %>%
  select(-c("dem_2p.x", "dem_2p.y", "adj_pred.x", "adj_pred.y"))

df[df$st_cd_fips == "1228", 'state'] = "Florida"
df[df$st_cd_fips == "1228", 'district_num'] = "28"

df <- df %>%
   mutate(pred_d = ifelse(adj_pred > 50, 1, 0))

df %>%
  filter(winner_d != pred_d) %>%
  select(state, district_num, dem_2p, adj_pred) %>%
  mutate(diff = dem_2p - adj_pred) %>%
  gt() %>%
  tab_header(title = "Incorrectly Predicted Districts",
             subtitle = "Prediction of Democratic Two-Party Vote-Share") %>%
  cols_label(
    state = "State",
    district_num = "District",
    dem_2p = "Actual Results", 
    adj_pred = "Prediction",
    diff = "Difference") %>%
  cols_align(align = "center") %>%
  fmt_percent(columns = c(dem_2p, adj_pred, diff), scale_values = FALSE) %>%
  tab_style(
    style = list(
      cell_fill(color = "lightblue"),
      cell_text()
      ),
    locations = cells_body(
      columns = diff,
      rows = diff >= 0
    )
  ) %>%
  tab_style(
    style = list(
      cell_fill(color = "lightcoral"),
      cell_text()
      ),
    locations = cells_body(
      columns = diff,
      rows = diff < 0
    )
  )
```

I do consider this a success! I actually had a 96% classification rate and miscalled less districts than both the Economist (18 miscalled districts), and 538 (23 miscalled districts). There appear to be some trends in the districts that I tended to miss. Many of the districts are very close - ME02, NE02, NM02, NC13, and VA02 all are within 2 percentage points of my predictions. In New York and California, however, I missed the prediction by as much as 18%. I am not alone in this - the districts that I miscalled were also districts that tend to be miscalled. New York performed particularly bad for Democrats due to mismanagement by the New York Democratic Party and court-ordered changes to redistricting. Similar issues in California also led to much fewer seats that predicted.

Below, I compare the districts that I called wrong to predictions from 538, The Economist, and the Cook Political Report. Here we see that many of the districts that I called wrong were similarly called wrong by other expert predictions. In particular, there are 7 districts that were called incorrectly across the board.

```{r}
fivethirtyeight <- c(1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1)
economist <- c(1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1)
cook <- c(1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1)

df %>%
  filter(winner_d != pred_d) %>%
  select(state, district_num) %>%
  cbind(fivethirtyeight) %>%
  cbind(economist) %>%
  cbind(cook) %>%
  gt() %>%
  tab_header(title = "Incorrectly Predicted Districts",
             subtitle = "Comparison with Experts") %>%
  cols_label(
    state = "State",
    district_num = "District",
    fivethirtyeight = "538 Error",
    economist = "Economist Error",
    cook = "CPR Error") %>%
  cols_align(align = "center")
```

Interestingly, the model without redistricting had a closer call in seat share by one seat, but had nearly twice as many incorrect seats called (33 miscalled seats). This indicates that, at least for my model, redistricting somewhat canceled out nationally in terms of partisan seats. Nonetheless, adding redistricting into my model improved the accuracy significantly.

# Prediction Trends

Below, I've plotted my predictions against the actual democratic two-party vote share. The obvious first thing to have done to improve the model would be to impute the values where the race was uncontested. The model did relatively well in predicting them - there were no unexpected uncontested races where the winner would not have been obviously from on party or another. This would have dropped the root mean square error to 5.78 from 9.89, nearly cutting it in half.

```{r, message = FALSE}
df_competitive <- df %>%
  filter(dem_2p != 0 & dem_2p != 100)

#sqrt(mean((df$dem_2p - df$adj_pred)^2))
#sqrt(mean((df_competitive$dem_2p - df_competitive$adj_pred)^2))

p1 <- ggplot(df, aes(x = adj_pred, y = dem_2p)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(x = "Prediction",
       y = "Actual Democratic Two-Party Vote-Share",
       caption = "y = 1.026x - 3.577\nAdjusted R^2 = 0.791",
       subtitle = "All Races") +
  theme_few()

p2 <- ggplot(df_competitive, aes(x = adj_pred, y = dem_2p)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(x = "Prediction",
       y = "Actual Democratic Two-Party Vote-Share",
       caption = "y = 0.868x - 5.451\nAdjusted R^2 = 0.902",
       subtitle = "Contested Races Only") +
  theme_few() + 
  ylim(0, 100)

grid.arrange(p1, p2, nrow = 1, top=textGrob("Trends in Prediction", gp = gpar(fontsize = 18)))
```

The other adjustment that I would have made to my model is to include some kind of variable for the quality of the campaigns that were run. Something that would've potentially captured the variation this cycle is the amount of capital that each of the campaigns had access to. In particular, mismanagement and lack of support typically means that there is a lack of funding for the campaigns. To test for this, I could retroactively test and see whether there is a relationship between how well a campaign did over a fundamentals model (with partisan lean from redistricting in the model) and how much money their campaign spent (looking at FEC data).






